{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Train/Test-split\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "\n",
    "As we've seen in the previous lecture, on way to evaluate a particular regression model's fit to our training data is to calculate the loss metric (either MAE, MSE or RMSE). We then say that the model with the smallest loss to fit the data the best.\n",
    "\n",
    "However, is the obtained results practically useful? As we'll se in this notebook, this is actually not the case. It turns out that the calculated loss metric on our training data is not a good indicator on a models usefullness. What we really want is a measure for how our model behaves on **new, unseen** data. But how can we assertain that? \n",
    "\n",
    "A well-working solution to this problem, as we'll see, is to split our data into two splits: a train- and a test split. Then we'll only use the train data to fit our model. The actual evaluation will then be done on the test data, which now simulates new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Importing needed libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define our loss-metrics. We'll use both MAE and RMSE here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(y_true, y_predictions):\n",
    "\n",
    "    '''Calculates the Mean Absolute Error between true values y and predicted y_hat'''\n",
    "    \n",
    "    initial_sum = sum([np.abs(y-y_hat) for y, y_hat in zip(y_true, y_predictions)])\n",
    "\n",
    "    mae = initial_sum/len(y_true)\n",
    "\n",
    "    mae_rounded = round(mae, 4)\n",
    "\n",
    "    return mae_rounded\n",
    "\n",
    "def RMSE(y_true, y_predictions):\n",
    "    \n",
    "    '''Calculates the Mean Squared Error between true values y and predicted y_hat'''\n",
    "    \n",
    "    initial_sum = sum([(y-y_hat)**2 for y, y_hat in zip(y_true, y_predictions)])\n",
    "\n",
    "    mse = initial_sum/len(y_true)\n",
    "\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    rmse_rounded = round(rmse, 4)\n",
    "\n",
    "    return rmse_rounded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and plot some dummy data for demonstrative purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "\n",
    "    y = -x**2 + 10*x\n",
    "\n",
    "    return y\n",
    "\n",
    "x_vals = np.linspace(0, 10, 20)\n",
    "y_vals = np.array([f(x) for x in x_vals])\n",
    "\n",
    "print(f'Number of training samples: {len(x_vals)}')\n",
    "\n",
    "sns.scatterplot(x=x_vals, y=y_vals);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some noise to our $y$-values, so that we get imperfect data. This is more realistic.\n",
    "\n",
    "In other words, for each $y$\n",
    "\n",
    "$$ y + \\epsilon $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\epsilon \\sim N(0, \\sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now let's add some noise to our y_vals, to mimic more realistic data\n",
    "\n",
    "noise_std = 3.0                                     # Adjust this value to control the amount of noise\n",
    "noise = np.random.normal(0, noise_std, len(y_vals)) # Generate noise from a normal distribution\n",
    "\n",
    "# add the generated noise to our y_vals\n",
    "y_vals = y_vals + noise\n",
    "\n",
    "# plot the results\n",
    "sns.scatterplot(x=x_vals, y=y_vals);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fitting models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now that we have dummy data. Let's do some quick regplots to see which degree polynomial naively fits best to this data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [1, 2, 12]\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 6), dpi=100)  # Adjust figsize as needed\n",
    "j = 0\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    sns.regplot(x=x_vals, y=y_vals, order=degree, ax=ax[i], ci=None)\n",
    "    ax[i].set_title(f\"Polynomial of degree: {degree}\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**:\n",
    "\n",
    "- The linear models doesn't seem to be a good model here\n",
    "- The polynomial of degree 2 seems to do a much better job\n",
    "- However, it looks as if the polynomial of degree 12 'fits' the data best, right? Since all $\\hat{y}$ are perfect, or very close, estimates of $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Naive loss-metric calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm the conclusions above by calculating MAE and MSE for the models we've plotted. We've used polynomials of order 1,2 and 12. \n",
    "\n",
    "In other words, we've assumed that:\n",
    "\n",
    "- For order 1:\n",
    "$$ f_{w,b}(x) = w_1 \\cdot x + b $$\n",
    "\n",
    "- For order 2:\n",
    "$$ f_{w,b}(x) = w_2 \\cdot x^2 + w_1 \\cdot x + b $$\n",
    "\n",
    "- For order 12:\n",
    "$$ f_{w,b}(x) = w_{12} \\cdot x^{12} + w_{11} \\cdot x^{11} + \\ldots + w_{1} \\cdot x + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by **defining** the models above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(x, w, b):    # a linear model is a polynomial of degree 1\n",
    "\n",
    "    y_hat = w*x+b\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "def polynomial_model_degree_2(x, w2, w1, b):\n",
    "\n",
    "    y_hat = w2*x**2 + w1*x+b\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "def polynomial_model_degree_12(x, w12, w11, w10, w9, w8, w7, w6, w5, w4, w3, w2, w1, b):\n",
    "    \n",
    "    y_hat = w12*x**12 + w11*x**11 + w10*x**10 + w9*x**9 +w8*x**8 + w7*x** 7 + w6*x**6 + w5*x**5 + w4*x**4 + w3*x**3 + w2*x**2 + w1*x + b\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can **polyfiting** in order to get the parameters. Thereafter, we'll **predict** the $\\hat{y}$ of each model, and finally use them to **calculate** our loss metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree 1 (linear model)\n",
    "\n",
    "w1, b = np.polyfit(x_vals, y_vals, deg=1)\n",
    "\n",
    "y_hats_linear_model = np.array([linear_model(x, w1, b) for x in x_vals])\n",
    "\n",
    "print('Polynomial of degree 1 (linear model):', end='\\n\\n')\n",
    "\n",
    "linear_model_MAE = MAE(y_vals, y_hats_linear_model)\n",
    "linear_model_RMSE = RMSE(y_vals, y_hats_linear_model)\n",
    "\n",
    "print('MAE  :', linear_model_MAE)\n",
    "print('RMSE :', linear_model_RMSE)\n",
    "\n",
    "sns.regplot(x=x_vals, y=y_vals, order=1, ci=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree 2 \n",
    "\n",
    "w2, w1, b = np.polyfit(x_vals, y_vals, deg=2)\n",
    "\n",
    "y_hats_polynomial_model_degree_2 = np.array([polynomial_model_degree_2(x, w2, w1, b) for x in x_vals])\n",
    "\n",
    "print('Polynomial of degree 2:', end='\\n\\n')\n",
    "\n",
    "polynomial_model_degree_2_MAE = MAE(y_vals, y_hats_polynomial_model_degree_2)\n",
    "polynomial_model_degree_2_RMSE = RMSE(y_vals, y_hats_polynomial_model_degree_2)\n",
    "\n",
    "print('MAE  :', polynomial_model_degree_2_MAE)\n",
    "print('RMSE :', polynomial_model_degree_2_RMSE)\n",
    "\n",
    "sns.regplot(x=x_vals, y=y_vals, order=2, ci=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree 12 \n",
    "\n",
    "w12, w11, w10, w9, w8, w7, w6, w5, w4, w3, w2, w1, b = np.polyfit(x_vals, y_vals, deg=12)\n",
    "\n",
    "y_hats_polynomial_model_degree_12 = np.array([polynomial_model_degree_12(x, w12, w11, w10, w9, w8, w7, w6, w5, w4, w3, w2, w1, b) for x in x_vals])\n",
    "\n",
    "print('Polynomial of degree 12:', end='\\n\\n')\n",
    "\n",
    "polynomial_model_degree_12_MAE = MAE(y_vals, y_hats_polynomial_model_degree_12)\n",
    "polynomial_model_degree_12_RMSE = RMSE(y_vals, y_hats_polynomial_model_degree_12)\n",
    "\n",
    "print('MAE  :', polynomial_model_degree_12_MAE)\n",
    "print('RMSE :', polynomial_model_degree_12_RMSE)\n",
    "\n",
    "sns.regplot(x=x_vals, y=y_vals, order=12, ci=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [1, 2, 12]\n",
    "all_maes = [linear_model_MAE, polynomial_model_degree_2_MAE, polynomial_model_degree_12_MAE]\n",
    "all_rmses = [linear_model_RMSE, polynomial_model_degree_2_RMSE, polynomial_model_degree_12_RMSE]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 6), dpi=100)  # Adjust figsize as needed\n",
    "j = 0\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    sns.regplot(x=x_vals, y=y_vals, order=degree, ax=ax[i], ci=None)\n",
    "    ax[i].set_title(f\"Polynomial of degree: {degree}\")\n",
    "    ax[i].set_xlabel(f\"MAE  : {all_maes[i]}\\nRMSE : {all_rmses[i]}\") \n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## This aint good enough though! \n",
    "\n",
    "Why? Because what we really want is to estimate the models performance on unseen data - **not** on the data that we have trained on! \n",
    "\n",
    "We're really only interested in how the model will be valuable to us when used in production, and we can't get a realiable estimate on that simply by how well the model fits on the data it has been trained on.\n",
    "\n",
    "In fact, as we'll see in the deep learning course, it is always possible to achieve a loss (e.g. MAE) of 0 on the data you've trained on.\n",
    "\n",
    "*Thus, in order to to simulate performance on data the model has not been trained on*, we will split our total training data into two sets\n",
    "\n",
    "1. a train set\n",
    "2. a test set\n",
    "\n",
    "The training set will be used exclusively to fit our model, while the testing set then will be what we calculate our loss on. If we get a good performance on the test set, we'll have good reasons to believe that the model is actually useful!\n",
    "\n",
    "**Let's demonstrate the concept**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very easy to split our data into a training- and testing set. Usually, we do a 80/20 random split.\n",
    "\n",
    "However, do not that the ratio of splits can be very different depending on circumstances - but 80/20 is a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use scikit-learn to split our data\n",
    "\n",
    "# scikit-learn is a Python library with tons of useful functions and methods to train ml-models\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_vals, y_vals, test_size=0.2)       # this function splits our data completely randomly each time you call it\n",
    "\n",
    "\n",
    "print('original data:')\n",
    "print(f'number of x_values: {len(x_vals)}')\n",
    "print(f'number of y_values: {len(y_vals)}', end='\\n\\n')\n",
    "\n",
    "print('train data:')\n",
    "print(f'number of x_values: {len(x_train)}')\n",
    "print(f'number of y_values: {len(y_train)}', end='\\n\\n')\n",
    "\n",
    "print('test data:')\n",
    "print(f'number of x_values: {len(x_test)}')\n",
    "print(f'number of y_values: {len(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize our splits\n",
    "\n",
    "plt.scatter(x_train, y_train, label='train set')\n",
    "plt.scatter(x_test, y_test, label='test set')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit our polynomials of degree 1, 2 and 12 using only data from the train set, and plot it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 6), dpi=100)  # Adjust figsize as needed\n",
    "j = 0\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    sns.regplot(x=x_train, y=y_train, order=degree, ax=ax[i], ci=None)   # note that we fit our models only on the train data here\n",
    "    ax[i].scatter(x_test, y_test, color='y')\n",
    "    ax[i].set_title(f\"Polynomial of degree: {degree}\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit our polynomials of degree 1, 2 and 12 using only data from the train set. Then, we'll evaluate our loss on both the train and the test set - the latter of which has not been seen on the test set\n",
    "\n",
    "This process is all going to be easier using scikit-learn, but we do this explicitly now just to showcase the process.\n",
    "\n",
    "To be clear, this is what we're going to do:\n",
    "\n",
    "For each polynoimal degree of 1,2 and 12:\n",
    "\n",
    "    1. Fit a model using only the training data\n",
    "    2. Use that model to predict y_hat for all x in the train and test data (the latter of which we havnt seen during training)\n",
    "    3. Calculate MAE and RMSE for both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree 1 (linear model)\n",
    "\n",
    "w1, b = np.polyfit(x_train, y_train, deg=1)    # note that we fit only on the train set here\n",
    "\n",
    "y_hats_train_linear_model = np.array([linear_model(x, w1, b) for x in x_train])   # predict y_hats for the train set\n",
    "y_hats_test_linear_model = np.array([linear_model(x, w1, b) for x in x_test])     # predict y_hats for the test set\n",
    "\n",
    "print('Polynomial of degree 1 (linear model):', end='\\n\\n')\n",
    "\n",
    "linear_model_train_MAE = MAE(y_train, y_hats_train_linear_model)       # calculate MAE for train set\n",
    "linear_model_train_RMSE = RMSE(y_train, y_hats_train_linear_model)     # calculate RMSE for train set\n",
    "\n",
    "linear_model_test_MAE = MAE(y_test, y_hats_test_linear_model)          # calculate MAE for test set\n",
    "linear_model_test_RMSE = RMSE(y_test, y_hats_test_linear_model)        # calculate RMSE for test set\n",
    "\n",
    "print('Train:')\n",
    "print('MAE :', linear_model_train_MAE)\n",
    "print('RMSE:', linear_model_train_RMSE, end='\\n\\n')\n",
    "\n",
    "print('Test:')\n",
    "print('MAE :', linear_model_test_MAE)\n",
    "print('RMSE:', linear_model_test_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree 2 \n",
    "\n",
    "w2, w1, b = np.polyfit(x_train, y_train, deg=2)     # note that we fit only on the train set here\n",
    "\n",
    "y_hats_train_polynomial_model_degree_2 = np.array([polynomial_model_degree_2(x, w2, w1, b) for x in x_train])  # predict y_hats for the train set\n",
    "y_hats_test_polynomial_model_degree_2 = np.array([polynomial_model_degree_2(x, w2, w1, b) for x in x_test])    # predict y_hats for the test set\n",
    "\n",
    "print('Polynomial of degree 2:', end='\\n\\n')\n",
    "\n",
    "polynomial_model_train_degree_2_MAE = MAE(y_train, y_hats_train_polynomial_model_degree_2)            # calculate MAE for train set\n",
    "polynomial_model_train_degree_2_RMSE = RMSE(y_train, y_hats_train_polynomial_model_degree_2)           # calculate MAE for test set\n",
    "\n",
    "polynomial_model_test_degree_2_MAE = MAE(y_test, y_hats_test_polynomial_model_degree_2)               # calculate MSE for train set\n",
    "polynomial_model_test_degree_2_RMSE = RMSE(y_test, y_hats_test_polynomial_model_degree_2)             # calculate MSE for test set\n",
    "\n",
    "print('Train:')\n",
    "print('MAE :', polynomial_model_train_degree_2_MAE)\n",
    "print('RMSE:', polynomial_model_train_degree_2_RMSE, end='\\n\\n')\n",
    "\n",
    "print('Test')\n",
    "print('MAE :', polynomial_model_test_degree_2_MAE)\n",
    "print('RMSE:', polynomial_model_test_degree_2_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree 12 \n",
    " \n",
    "\n",
    "w12, w11, w10, w9, w8, w7, w6, w5, w4, w3, w2, w1, b = np.polyfit(x_train, y_train, deg=12)   # note that we fit only on the train set here\n",
    "\n",
    "# predict y_hats for the train set\n",
    "# predict y_hats for the test set\n",
    "\n",
    "y_hats_train_polynomial_model_degree_12 = np.array([polynomial_model_degree_12(x, w12, w11, w10, w9, w8, w7, w6, w5, w4, w3, w2, w1, b) for x in x_train])\n",
    "y_hats_test_polynomial_model_degree_12 = np.array([polynomial_model_degree_12(x, w12, w11, w10, w9, w8, w7, w6, w5, w4, w3, w2, w1, b) for x in x_test])\n",
    "\n",
    "print('Polynomial of degree 12:', end='\\n\\n')\n",
    "\n",
    "polynomial_model_train_degree_12_MAE = MAE(y_train, y_hats_train_polynomial_model_degree_12)            # calculate MAE for train set\n",
    "polynomial_model_train_degree_12_RMSE = RMSE(y_train, y_hats_train_polynomial_model_degree_12)          # calculate MAE for test set\n",
    "\n",
    "polynomial_model_test_degree_12_MAE = MAE(y_test, y_hats_test_polynomial_model_degree_12)               # calculate MSE for train set\n",
    "polynomial_model_test_degree_12_RMSE = RMSE(y_test, y_hats_test_polynomial_model_degree_12)             # calculate MSE for test set\n",
    "\n",
    "print('Train:')\n",
    "print('MAE :', polynomial_model_train_degree_12_MAE)\n",
    "print('RMSE:', polynomial_model_train_degree_12_RMSE, end='\\n\\n')\n",
    "\n",
    "print('Test:')\n",
    "print('MAE :', polynomial_model_test_degree_12_MAE)\n",
    "print('RMSE:', polynomial_model_test_degree_12_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results so that we can see what we've done\n",
    "\n",
    "degrees = [1, 2, 12]\n",
    "all_train_maes = [linear_model_train_MAE, polynomial_model_train_degree_2_MAE, polynomial_model_degree_12_MAE]\n",
    "all_train_mses = [linear_model_train_RMSE, polynomial_model_train_degree_2_RMSE, polynomial_model_degree_12_RMSE]\n",
    "\n",
    "all_test_maes = [linear_model_test_MAE, polynomial_model_test_degree_2_MAE, polynomial_model_test_degree_12_MAE]\n",
    "all_test_mses = [linear_model_test_RMSE, polynomial_model_test_degree_2_RMSE, polynomial_model_test_degree_12_RMSE]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 6), dpi=100) \n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    sns.regplot(x=x_train, y=y_train, order=degree, ax=ax[i], ci=None)\n",
    "    ax[i].scatter(x_test, y_test, color='y')\n",
    "    ax[i].set_title(f\"Polynomial of degree: {degree}\")\n",
    "    ax[i].set_xlabel(f\"Train:\\n\\nMAE: {all_train_maes[i]}\\nRMSE: {all_train_mses[i]}\\n\\nTest:\\n\\nMAE: {all_test_maes[i]}\\nRMSE: {all_test_mses[i]}\") \n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen that a low loss on only the train set is **not** a good indicator of performance on unseen data - which is what we require for the model to be actually useful.\n",
    "\n",
    "\n",
    "**Underfit**\n",
    "\n",
    "A model that has too little flexibility in relation to the data we train it on, will usually have poor performance on both the train and test data. We then say that the model is underfit. In our example, this is the case for the linear model.\n",
    "\n",
    "**Good fit**\n",
    "\n",
    "A model that has just the right amount of flexilibity will generally achieve good performance on both the train and test data. The goal in ML is thus to fit models to our data with as low test set loss as possible - since this likely entails the best performance on unseen data. In our example, this is the case for the polynomial model of degree 2.\n",
    "\n",
    "**Overfit**\n",
    "\n",
    "As a model becomes more and more flexible, i.e, it's ability to fit training data increases - we risk ending up with a model that is *overfit*. An overfit model is characterised by very good performance on the train set, but severly lacking performance on the test set. In our example, this is the case for the polynomial model of degree 12.\n",
    "\n",
    "A model that is too flexible, and therefore tend to overfit, is also said to have high $variance$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**\n",
    "\n",
    "Go through the notebook and make sure you firmly understand:\n",
    "\n",
    "* The code\n",
    "* The concepts explained\n",
    "\n",
    "**Task 2**\n",
    "\n",
    "Fit polynomial models of degree 4, 6, 8 on the train set, and calculcate MAE and RMSE on the test set.\n",
    "How do their performance compare with the models we've used so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "databehandling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
